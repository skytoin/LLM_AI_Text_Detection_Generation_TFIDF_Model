# LLM_AI_Text_Detection_Generation_TFIDF_Model

This Notebook is embarking on an important project to develop a model that distinguishes between essays written by 
middle and high school students and those generated by large language models (LLMs). The increasing use of LLMs has 
sparked a debate about their role in academic and creative fields. Particularly in education, there's a growing concern 
about their impact on student learning. While some educators see LLMs as potential tools for enhancing students' 
writing abilities, others worry about their negative effects, especially regarding skill development.

A primary academic concern surrounding LLMs is their potential to facilitate plagiarism. These models, trained on 
extensive datasets of text and code, are adept at creating text remarkably similar to human writing. This capability 
raises the possibility of students using LLMs to produce essays, thereby bypassing essential learning experiences. 
Such actions not only undermine educational integrity but also impede the development of critical thinking and writing skills.

The goal of this notebook to build classification model without using LLMs or Deep Learning models to classify text. 
By identifying unique characteristics of LLM-generated texts, we learn how to destinguish who or what wrote particular text. 
Our approach involves using texts of moderate length covering various topics, which are reflective of typical scenarios 
where LLM-generated content might be used. The challenge also includes dealing with texts produced by multiple, unknown 
generative models. The goal is to encourage the development of detection methods that are effective across different LLMs, 
thereby contributing to a broader understanding and capability in this emerging field.**

This dataset contains around 44000  samples of students' essays and different LLMs generated texts essays.
https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset
