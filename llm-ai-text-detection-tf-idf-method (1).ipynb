{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f19d7cb",
   "metadata": {
    "papermill": {
     "duration": 0.007671,
     "end_time": "2024-01-27T18:30:48.247567",
     "exception": false,
     "start_time": "2024-01-27T18:30:48.239896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ****LLM AI Text Detection TF-IDF Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd951333",
   "metadata": {
    "papermill": {
     "duration": 0.006379,
     "end_time": "2024-01-27T18:30:48.261248",
     "exception": false,
     "start_time": "2024-01-27T18:30:48.254869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* This Notebook is embarking on an important project to develop a model that distinguishes between essays written by middle and high school students and those generated by large language models (LLMs). The increasing use of LLMs has sparked a debate about their role in academic and creative fields. Particularly in education, there's a growing concern about their impact on student learning. While some educators see LLMs as potential tools for enhancing students' writing abilities, others worry about their negative effects, especially regarding skill development.\n",
    "\n",
    "* A primary academic concern surrounding LLMs is their potential to facilitate plagiarism. These models, trained on extensive datasets of text and code, are adept at creating text remarkably similar to human writing. This capability raises the possibility of students using LLMs to produce essays, thereby bypassing essential learning experiences. Such actions not only undermine educational integrity but also impede the development of critical thinking and writing skills.\n",
    "\n",
    "* The goal of this notebook to build classification model without using LLMs or Deep Learning models to classify text. By identifying unique characteristics of LLM-generated texts, we learn how to destinguish who or what wrote particular text. Our approach involves using texts of moderate length covering various topics, which are reflective of typical scenarios where LLM-generated content might be used. The challenge also includes dealing with texts produced by multiple, unknown generative models. The goal is to encourage the development of detection methods that are effective across different LLMs, thereby contributing to a broader understanding and capability in this emerging field.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bebda575",
   "metadata": {
    "_cell_guid": "d8a44a62-421f-4a2d-b0f0-03b2cc55d275",
    "_uuid": "ef5866fd-548e-492e-af89-92fa02157751",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-27T18:30:48.276029Z",
     "iopub.status.busy": "2024-01-27T18:30:48.275577Z",
     "iopub.status.idle": "2024-01-27T18:30:57.466951Z",
     "shell.execute_reply": "2024-01-27T18:30:57.465843Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 9.202894,
     "end_time": "2024-01-27T18:30:57.470347",
     "exception": false,
     "start_time": "2024-01-27T18:30:48.267453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1621167165.py:31: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  from tqdm._tqdm_notebook import tqdm_notebook\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    "    SentencePieceBPETokenizer\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from joblib import dump\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1238d4e",
   "metadata": {
    "_cell_guid": "870ec4ef-a95f-4c46-81bd-4304efb9b783",
    "_uuid": "549b8011-10a1-4ae5-bdfa-0da946add658",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-27T18:30:57.486655Z",
     "iopub.status.busy": "2024-01-27T18:30:57.485842Z",
     "iopub.status.idle": "2024-01-27T18:30:57.491763Z",
     "shell.execute_reply": "2024-01-27T18:30:57.490504Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.016708,
     "end_time": "2024-01-27T18:30:57.494170",
     "exception": false,
     "start_time": "2024-01-27T18:30:57.477462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59846d87",
   "metadata": {
    "_cell_guid": "d7a2a347-4646-4511-9feb-76d37d067c93",
    "_uuid": "a51a0153-8275-4597-8b56-c1e7101ce97d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-27T18:30:57.510699Z",
     "iopub.status.busy": "2024-01-27T18:30:57.510211Z",
     "iopub.status.idle": "2024-01-27T18:30:57.535022Z",
     "shell.execute_reply": "2024-01-27T18:30:57.533827Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.036233,
     "end_time": "2024-01-27T18:30:57.537704",
     "exception": true,
     "start_time": "2024-01-27T18:30:57.501471",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    pass\n",
    "else:\n",
    "    submit_df = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n",
    "    submit_df.to_csv('submission.csv', index=False)\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f605f",
   "metadata": {
    "_cell_guid": "16098475-61b7-4dba-b258-b89440b8fd8f",
    "_uuid": "dbc3836c-6eec-4811-815f-8f4e8d1f6312",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-22T18:26:12.467404Z",
     "iopub.status.busy": "2024-01-22T18:26:12.466617Z",
     "iopub.status.idle": "2024-01-22T18:26:15.540379Z",
     "shell.execute_reply": "2024-01-22T18:26:15.539072Z",
     "shell.execute_reply.started": "2024-01-22T18:26:12.467347Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "submit_df = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n",
    "train_df = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb63c3d",
   "metadata": {
    "_cell_guid": "6d88ffd9-73ee-4e66-aad3-cf23bc869d4b",
    "_uuid": "5e061d2d-7d71-4e39-979b-552cde46ab52",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-22T18:26:43.075118Z",
     "iopub.status.busy": "2024-01-22T18:26:43.074133Z",
     "iopub.status.idle": "2024-01-22T18:26:43.173217Z",
     "shell.execute_reply": "2024-01-22T18:26:43.171843Z",
     "shell.execute_reply.started": "2024-01-22T18:26:43.075072Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop_duplicates(subset=['text'])\n",
    "train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98482271",
   "metadata": {
    "_cell_guid": "2760c643-af20-4535-86a3-db28e5c2cbac",
    "_uuid": "7d94829c-1d11-418a-8b03-31b9277b33db",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-22T18:26:46.057023Z",
     "iopub.status.busy": "2024-01-22T18:26:46.056029Z",
     "iopub.status.idle": "2024-01-22T18:26:46.061933Z",
     "shell.execute_reply": "2024-01-22T18:26:46.060573Z",
     "shell.execute_reply.started": "2024-01-22T18:26:46.056984Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IS_LOWERCASE = False\n",
    "MAX_VOCAB = 42000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf825f30",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Building Byte-Pair Encoding Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a91fa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "* In this section of the code, we set up a Byte-Pair Encoding (BPE) tokenizer, an advanced technique widely used in natural language processing for breaking down text into subword units. This approach is particularly effective in handling out-of-vocabulary words, as it splits them into recognizable subword segments.\n",
    "\n",
    "* The configuration of the tokenizer includes several specialized tokens, specifically [UNK], [PAD], [CLS], [SEP], and [MASK], each serving a unique purpose in the tokenization process. For instance, [UNK] represents unknown words, while [PAD] is used for padding sequences to a uniform length.\n",
    " \n",
    "* To optimize the tokenizer for BPE, we implement normalization and pre-tokenization processes. These steps ensure that the text is consistently formatted and prepared for efficient subword segmentation.\n",
    " \n",
    "* The training of the tokenizer is conducted iteratively using portions of the dataset. This method allows for gradual learning and adaptation to the linguistic characteristics present in the data. Once trained, the tokenizer is encapsulated within a PreTrainedTokenizerFast object. This wrapper enhances the tokenizer's performance, enabling rapid and efficient tokenization.\n",
    " \n",
    "* The final stage involves applying this tokenizer to both the test and training datasets. This step ensures that the text in these datasets is broken down into a format suitable for subsequent processing, maintaining consistency in tokenization across both sets of data. By doing so, we lay the groundwork for more effective and uniform analysis and modeling of the text data in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d117446",
   "metadata": {
    "_cell_guid": "0569c914-c470-41f1-acff-238d72ef8411",
    "_uuid": "258bd5a1-c809-4116-8ce7-d7777ba44cdf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-22T18:26:54.778185Z",
     "iopub.status.busy": "2024-01-22T18:26:54.777160Z",
     "iopub.status.idle": "2024-01-22T18:29:33.553802Z",
     "shell.execute_reply": "2024-01-22T18:29:33.552325Z",
     "shell.execute_reply.started": "2024-01-22T18:26:54.778144Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating Byte-Pair Encoding tokenizer\n",
    "bpe_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "bpe_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if IS_LOWERCASE else [])\n",
    "bpe_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "bpe_special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "bpe_trainer = trainers.BpeTrainer(vocab_size=MAX_VOCAB, special_tokens=bpe_special_tokens)\n",
    "bpe_dataset = Dataset.from_pandas(test_df[['text']])\n",
    "\n",
    "def bpe_corpus_iter(): \n",
    "    for idx in range(0, len(bpe_dataset), 1000):\n",
    "        yield bpe_dataset[idx : idx + 1000][\"text\"]\n",
    "bpe_tokenizer.train_from_iterator(bpe_corpus_iter(), trainer=bpe_trainer)\n",
    "fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=bpe_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "test_texts_tokenized = []\n",
    "\n",
    "for text_item in tqdm(test_df['text'].tolist()):\n",
    "    test_texts_tokenized.append(fast_tokenizer.tokenize(text_item))\n",
    "\n",
    "train_texts_tokenized = []\n",
    "\n",
    "for text_item in tqdm(train_df['text'].tolist()):\n",
    "    train_texts_tokenized.append(fast_tokenizer.tokenize(text_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9685d168",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852cebae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "* In this segment of the code, we're dealing with the implementation of the TF-IDF (Term Frequency-Inverse Document Frequency) vectorization, a crucial step in text processing that emphasizes the importance of words in a corpus. The technique involves calculating the frequency of a word in a document relative to its frequency across all documents, helping to highlight words that are unique to certain documents.\n",
    "\n",
    "* The TF-IDF vectorizer is configured to analyze word combinations within a range of 3 to 5 words (n-grams). This range allows the model to consider not only individual words but also phrases up to five words long, providing a more nuanced understanding of the text. The setup includes sublinear term frequency scaling, a method that moderates the influence of frequently occurring terms, and unicode accent stripping, which simplifies text by removing accents.\n",
    "\n",
    "* To maintain precise control over the tokenization and preprocessing steps, custom functions are employed. These functions override the default behaviors, ensuring that the text is processed exactly as intended. This level of control is particularly important when dealing with complex datasets or specific tokenization protocols.\n",
    "\n",
    "* After configuring the vectorizer, it is trained (or \"fitted\") using the already tokenized test data. This step involves the vectorizer learning the vocabulary of the test set, which includes identifying the most relevant terms and phrases as per the TF-IDF criteria.\n",
    "\n",
    "* Subsequently, this learned vocabulary is utilized to initialize a new TF-IDF vectorizer. This new vectorizer is then applied to both the training and test datasets, transforming them into a numerical format that reflects the importance of each term in the context of the larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7a4f9",
   "metadata": {
    "_cell_guid": "72b49330-8897-4077-93e8-22ce9faee80a",
    "_uuid": "0d2d5b26-20ff-4008-aa14-6130a291cb71",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-22T18:36:47.785831Z",
     "iopub.status.busy": "2024-01-22T18:36:47.785289Z",
     "iopub.status.idle": "2024-01-22T18:41:48.690833Z",
     "shell.execute_reply": "2024-01-22T18:41:48.689688Z",
     "shell.execute_reply.started": "2024-01-22T18:36:47.785795Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def no_process(text):\n",
    "    return text\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, analyzer = 'word',\n",
    "    tokenizer = no_process,\n",
    "    preprocessor = no_process,\n",
    "    token_pattern = None, strip_accents='unicode')\n",
    "\n",
    "tfidf_vectorizer.fit(test_texts_tokenized)\n",
    "\n",
    "# Getting vocab\n",
    "tfidf_vocab = tfidf_vectorizer.vocabulary_\n",
    "\n",
    "print(tfidf_vocab)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=tfidf_vocab,\n",
    "                            analyzer = 'word',\n",
    "                            tokenizer = no_process,\n",
    "                            preprocessor = no_process,\n",
    "                            token_pattern = None, strip_accents='unicode'\n",
    "                            )\n",
    "\n",
    "tf_train_data = tfidf_vectorizer.fit_transform(train_texts_tokenized)\n",
    "tf_test_data = tfidf_vectorizer.transform(test_texts_tokenized)\n",
    "\n",
    "del tfidf_vectorizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cfff7b",
   "metadata": {
    "_cell_guid": "78020aa4-f2c0-4116-82d9-5674e688d774",
    "_uuid": "31552fd2-9d69-4d92-86f8-83fa4a44d023",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels = train_df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d4858",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# **Ensemble Learning with Multiple Classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c732c9b",
   "metadata": {
    "_cell_guid": "32401fd7-9ce7-4bb8-86c5-efb36797c0bc",
    "_uuid": "d9bf2ac6-bdf7-4154-9aa0-0148af4add6e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-22T18:42:12.871174Z",
     "iopub.status.busy": "2024-01-22T18:42:12.870657Z",
     "iopub.status.idle": "2024-01-22T18:42:13.846162Z",
     "shell.execute_reply": "2024-01-22T18:42:13.844092Z",
     "shell.execute_reply.started": "2024-01-22T18:42:12.871135Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB(alpha=0.02)\n",
    "sgd_clf = SGDClassifier(max_iter=9000, tol=1e-4, loss=\"modified_huber\") \n",
    "lgb_params = {'n_iter': 3000, 'verbose': -1, 'objective': 'cross_entropy', 'metric': 'auc',\n",
    "              'learning_rate': 0.00581909898961407, 'colsample_bytree': 0.78,\n",
    "              'colsample_bynode': 0.8, 'lambda_l1': 4.562963348932286, \n",
    "              'lambda_l2': 2.97485, 'min_data_in_leaf': 115, 'max_depth': 23, 'max_bin': 898}\n",
    "lgb_model = LGBMClassifier(**lgb_params)\n",
    "cat_model = CatBoostClassifier(iterations=3000,\n",
    "                       verbose=0,\n",
    "                       l2_leaf_reg=6.6591278779517808,\n",
    "                       learning_rate=0.005599066836106983,\n",
    "                       subsample = 0.4,\n",
    "                       loss_function = 'CrossEntropy')\n",
    "model_weights = [0.07, 0.31, 0.31, 0.31]\n",
    "\n",
    "voting_ensemble = VotingClassifier(estimators=[('mnb', nb_classifier),\n",
    "                                               ('sgd', sgd_clf),\n",
    "                                               ('lgb', lgb_model), \n",
    "                                               ('cat', cat_model)\n",
    "                                              ],\n",
    "                                   weights=model_weights, voting='soft', n_jobs=-1)\n",
    "\n",
    "if len(test_df.text.values) <= 5:\n",
    "    submit_df.to_csv('submission.csv', index=False)\n",
    "else:\n",
    "    print('Big Apple')\n",
    "    voting_ensemble.fit(tf_train_data, train_labels)\n",
    "    gc.collect()\n",
    "\n",
    "final_bpe_predictions = voting_ensemble.predict_proba(tf_test_data)[:,1]\n",
    "\n",
    "del test_texts_tokenized, train_texts_tokenized, bpe_dataset, bpe_tokenizer, fast_tokenizer\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7f311",
   "metadata": {
    "_cell_guid": "f6bdc02b-356c-4161-b779-3f4bad680866",
    "_uuid": "21c5779b-d913-4955-a620-1e2a329c4938",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit_df['generated'] = final_bpe_predictions\n",
    "submit_df.to_csv('submission.csv', index=False)\n",
    "submit_df"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 3960967,
     "sourceId": 6901341,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3942644,
     "sourceId": 6890527,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4005256,
     "sourceId": 6977472,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3945154,
     "sourceId": 6865136,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.336255,
   "end_time": "2024-01-27T18:30:59.171191",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-27T18:30:44.834936",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
